@article{Fellner2013,
  title={Testing enforcement strategies in the field: Threat, moral appeal and social information},
  author={Fellner, Gerlinde and Sausgruber, Rupert and Traxler, Christian},
  journal={Journal of the European Economic Association},
  volume={11},
  number={3},
  pages={634--660},
  year={2013},
  publisher={Oxford University Press}
}

@article{Ferman2019,
  title={A simple way to assess inference methods},
  author={Ferman, Bruno},
  journal={arXiv preprint arXiv:1912.08772},
  year={2019}
}

@article{Dwenger2016,
  title={Extrinsic and intrinsic motivations for tax compliance: Evidence from a field experiment in Germany},
  author={Dwenger, Nadja and Kleven, Henrik and Rasul, Imran and Rincke, Johannes},
  journal={American Economic Journal: Economic Policy},
  volume={8},
  number={3},
  pages={203--32},
  year={2016}
}

@article{Luttmer2014,
  title={Tax morale},
  author={Luttmer, Erzo FP and Singhal, Monica},
  journal={Journal of economic perspectives},
  volume={28},
  number={4},
  pages={149--68},
  year={2014}
}

@article{Naritomi2019,
  title={Consumers as tax auditors},
  author={Naritomi, Joana},
  journal={American Economic Review},
  volume={109},
  number={9},
  pages={3031--72},
  year={2019}
}

@article{Duflo2007,
  title={Using randomization in development economics research: A toolkit},
  author={Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  journal={Handbook of development economics},
  volume={4},
  pages={3895--3962},
  year={2007},
  publisher={Elsevier}
}

@article{Ai2003,
  title={Interaction terms in logit and probit models},
  author={Ai, Chunrong and Norton, Edward C},
  journal={Economics letters},
  volume={80},
  number={1},
  pages={123--129},
  year={2003},
  publisher={Elsevier}
}

@book{Imbens2015,
  title={Causal inference in statistics, social, and biomedical sciences},
  author={Imbens, Guido W and Rubin, Donald B},
  year={2015},
  publisher={Cambridge University Press}
}

@article{Bowles2001,
Author = {Bowles, Samuel and Gintis, Herbert and Osborne, Melissa},
Title = {The Determinants of Earnings: A Behavioral Approach},
Journal = {Journal of Economic Literature},
Volume = {39},
Number = {4},
Year = {2001},
Month = {December},
Pages = {1137-1176},
DOI = {10.1257/jel.39.4.1137},
URL = {https://www.aeaweb.org/articles?id=10.1257/jel.39.4.1137}}

@book{Ashenfelter2010,
  title={Handbook of labor economics},
  author={Ashenfelter, Orley and Card, David},
  year={2010},
  publisher={Elsevier}
}

@article{Imbens2015b,
  title={Matching methods in practice: Three examples},
  author={Imbens, Guido W},
  journal={Journal of Human Resources},
  volume={50},
  number={2},
  pages={373--419},
  year={2015},
  publisher={University of Wisconsin Press}
}

@book{Angrist2008,
  title={Mostly harmless econometrics: An empiricist's companion},
  author={Angrist, Joshua D and Pischke, J{\"o}rn-Steffen},
  year={2008},
  publisher={Princeton university press}
}

@article{Belloni2014,
  title={Inference on treatment effects after selection among high-dimensional controls},
  author={Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
  journal={The Review of Economic Studies},
  volume={81},
  number={2},
  pages={608--650},
  year={2014},
  publisher={Oxford University Press}
}

@article{Wager2018,
  title={Estimation and inference of heterogeneous treatment effects using random forests},
  author={Wager, Stefan and Athey, Susan},
  journal={Journal of the American Statistical Association},
  volume={113},
  number={523},
  pages={1228--1242},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{Athey2019,
  title={Estimating treatment effects with causal forests: An application},
  author={Athey, Susan and Wager, Stefan},
  journal={arXiv preprint arXiv:1902.07409},
  year={2019}
}

@article{Breiman2001,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{Athey2019b,
  title={Generalized random forests},
  author={Athey, Susan and Tibshirani, Julie and Wager, Stefan and others},
  journal={The Annals of Statistics},
  volume={47},
  number={2},
  pages={1148--1178},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@article{Chernozhukov2018a,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = "{Double/debiased machine learning for treatment and structural parameters}",
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}
