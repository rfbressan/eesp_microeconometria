---
title: "Microeconometrics I"
subtitle: "Homework I"
author: 
  - "Professor: Bruno Ferman"
  - "Student: Rafael F. Bressan"
date: "`r Sys.Date()`"
output: 
  # bookdown::html_document2:
  #   number_sections: false
  #   highlight: default
  bookdown::pdf_document2:
    number_sections: false
    highlight: default
    # keep_tex: true
    toc: false
bibliography: references.bib
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
tex_output <- knitr::is_latex_output()

library(sandwich)
library(lmtest)
library(fixest)
library(paramtest)
library(dplyr)
library(tidyr)
library(kableExtra)
load("input/workspace.RData")
```

### Choose a published paper based on a field experiment in which the dataset is available online. Once you decide on your paper, sign up at this link. It must be a different paper for each student.

I have chosen the following paper _"Testing Enforcement Strategies in the Field: Threat, Moral Appeal and Social Information"_ from the Journal of the European Economic Association, by  Gerlinde Fellner, Rupert Sausgruber, and Christian Traxler, published in June 2013 [@Fellner2013].

First a note on why this paper has been chosen. It is in my line of work to understand tax compliance and the motives for avoidance. Field experiments can be used, and indeed they are, as there is abundant literature on this topic, to understand the reasons tax payers have to comply or to defraud (or at least omit themselves) the tax system. Although many journals do not provide open access to published article's data, the journals from the American Economic Association, Econometrica and (I recently discovered) the European Economic Association do provide access to supplementary material, including datasets and code such that other researchers are able to reproduce the results of any published article. I first tried to search papers on the journals from AEA and found some interesting ones [@Naritomi2019; @Dwenger2016; @Luttmer2014], but, even though all these papers were supposed to provide access to their datasets, due to the confidential nature of these data, the authors opted not to provide them. Therefore, I end up in the Journal of the European Economic Association and found the above paper which, fortunately, had all supplementary material available.

### Briefly summarize the research question in the paper, and why it is interesting. What is the main parameter of interest?

The authors are interested in evaluating several strategies to enforce tax law compliance. Considering the majority of taxes nowadays are the self-reporting type, the tax authority is interested in knowing what does it take for a person or a company to correctly self-report the due tax and pay it. Moreover, if the authority is suspicious of evasion, what is the most effective way to collect the due but unpaid taxes? To answer these questions it important to know what makes the agents correctly report their due taxes, and pay it, but also to understand the methods the authority have to collect from evading agents. Tax audits are expensive, both in terms of wages paid for the tax authority employees and time consumed during an audit. Also, audit are not easily scalable, since every audit has its own nuances. If, for example, a mailing can be sent to potential evaders pointing out the risks associated with non-compliance and it has a deterrent effect, this mailing can be preferred as a first strategy to collect unpaid taxes before a full-blown tax audit be unleashed upon the agent. But the question still remains, what should be the contents of this letter such that people are more susceptible to comply after receiving it?

Therefore, in their study the authors are mainly interested in evaluating the different effects of _wording_ in the letter contents. They use as baseline the tax authority's standard letter. They **extended** the letter along three dimensions: a threat (of legal and financial consequences); a moral appealing (behavioral motives); and social information (stressing the high level of compliance). Their main parameter of interest are fee payments within 50 days of the experiment.

### Briefly describe the design of the experiment.

The authors experimentally manipulated mailings sent by Austrian tax authority to more than fifty thousands potential evaders. It is important to notice this list is not randomly sampled from the whole Austrian population, but instead is a selected sample of potential evaders. Therefore, the experiment is randomized only within the listed tax payers. A cover letter, an information sheet and a response and registration form with prepaid envelope were the contents of the mailing.

In the standard letter, it is explained that the recipient is not registered as a fee payer, which is required by law by anyone who has a TV or radio and is asked to clarify the facts to the tax authority within 14 days. Legal notice is given, like the size of the fee and the fine that can be imposed in case of detection.

The treatments, which are the extensions made to the baseline letter, are randomized among the listed evaders. There are 3 extensions made, the threat contains a paragraph stressing the significant detection risk and the consequences of noncompliance. The social information treatment include a paragraph with information of the actual level of compliance, 94\%. Finally the moral appeal treatment extended the baseline letter pointing out that "who do not conscientiously register [...] harm all honest households. Hence, registering is also a matter of fairness".

The listed potential evaders amounted to more than fifty thousand and comprise only those who had not received any prior mailing. A 5\% random subsample was set aside to be the no-mailing group (T0). The remaining recipients were randomized into 6 treatment buckets (T1-T6) as presented in the table \@ref(tab:buckets) bellow.

```{r buckets, results = "asis"}
kbl(buckets, booktabs = TRUE,
    caption = "Control and treatment buckets.") %>% 
  kable_classic(full_width = FALSE)
```

Considering that mailing is not a high cost treatment, specially for a government agency, we should expect a more balanced proportion of individuals in control and treatment groups. @Duflo2007 provides the following optimal allocation rule:

$$\frac{P}{1-P}=\sqrt{\frac{c_c}{c_t}}$$

where $P$ is the proportion of treated individuals, $c_t$ is the cost of treatment and $c_c$ the cost of controlling. While one can argument the cost of controlling in this setup is essentially zero, which would let the researchers choose a low value for $P$ if the experiment was budget constrained, this is not entirely correct. Much of the cost of this type of experiment falls upon gathering the information and compiling the list of potential evaders. The additional cost of mailing a random sample from this list is marginal, thus, more balanced groups would yield a lower $MDE$ at virtually the same cost.

### What is the minimum detectable effect for the main parameter of interest? Did the study have adequate power? (you can use the authors' data to answer this question)

Let's first calculate MDE for the effect of any treatment (T1-T6) comparing to the control group (T0). The proportion of treated individuals in the study is 0.949 for a total number of individuals of 50,498. Ideally one should NOT first collect the data, then run regressions to estimate the error's variance, but we are not in a perfect world.


\begin{equation}
Y_i=\alpha+\beta T_i + \varepsilon_i
(\#eq:reg-basic)
\end{equation}


Where $Y_i$ is the outcome variable, a dummy equal to 1 if individual registered (i.e. started paying the fee) within the first 50 days after sending the mailing, and $T_i$ is a treatment indicator. The $MDE$ is given by the following equation:


\begin{equation}
MDE=(t_{1-\kappa}+t_{\alpha/2})\sqrt{\frac{1}{P(1-P)}}\sqrt{\frac{\sigma^2}{N}}(\#eq:mde)
\end{equation}


thus, for a given significance level $\alpha$ and desired power $\kappa$ the $MDE$ is determined by the experiment design. The whole term inside the square root is the treatment effect's standard error, $\sqrt{var(\hat\beta|T)}$, which we have a robust estimation from above regression. Choosing power to be 80\% and significance level at 5\% for the two-sided test where $H0: \beta = 0$, we have that $MDE=$ `r format(MDE, digits = 4)`. Compare this value with the results from regression and we expect the experiment were able to capture the treatment effect with enough power.

```{r reg-sum, results='asis', warning=FALSE}
broom::tidy(reg_fix) %>% 
  kbl(col.names = c("", "Estimate", "Std.Error", "t-statistic", "P-value"),
      digits = 4, booktabs = TRUE,
      caption = "Simple regression for mailing effect.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE)
```

We can also make use of simulation in order to estimate the experiment's power. We already have an initial guess for $MDE$ thus, our simulations run through values around this guess. For each value of treatment effect imposed on the model, we run one thousand regressions on resampled[^1] data and perform the hypothesis test of zero effect. The average proportion of rejections for this set is the experiment power for that specific treatment effect. Table below shows the results for different values of effect.

[^1]: The sampling is made with replacement over _the entire_ data set, in order to preserve the assignment mechanism. After this shuffling an "artificial" outcome is created based on a binomial model.

```{r main-power, results = "asis", cache=TRUE}
mail_pwr_t <- t(mail_pwr)
row.names(mail_pwr_t) <- c("Effect", "Power")
mail_pwr_t %>%
  kbl(digits = 4, booktabs = TRUE, 
      row.names = TRUE,
      caption = "Experiment power and minimum detectable effect.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE)
```

Now, to test the power of all other treatments, in the full setup conducted by the authors we need to estimate the following regression:


\begin{equation}
  P_{i}^{r e g}=\alpha+\beta_{0} \text { Mailing}_{i}+\beta_{1} \text { Threat}_{i}+\beta_{2} \text { Moral}_{i}+\beta_{3} \text { Info}_{i}+\varepsilon_{i}\\
(\#eq:reg-full)
\end{equation}


where $P_i^{reg}$ is the same outcome as before, registration to pay the fee after 50 days; Mailing is the main treatment, receiving or not a letter; and Threat, Moral, and Info are dummy variables indicating the contents of the letter. In the paper they also allow for Threat to interact with the two other mailing contents to get all buckets, T1-T6. When running the regression in equation \@ref(eq:reg-full) we get the following results:

```{r reg-full, results = "asis", warning=FALSE}
broom::tidy(reg_full) %>% 
  mutate(term = c("(Intercept)", "Mailing", "Threat", "Moral", "Info")) %>% 
  kbl(col.names = c("", "Estimate", "Std.Error", "t-statistic", "P-value"),
      digits = 4, booktabs = TRUE,
      caption = "Full regression over four treatment types.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE)
```

Considering we have already shown the experiment has a $MDE$ much lower than the estimated effect for mailing, hence the experiment is adequately powered, we will now focus on the power for each coefficient. Simulations will held fixed all other parameters values at zero[^2] and simulate the parameter of interest around its estimated value on regression \@ref(eq:reg-full) to compute its power. The results for Threat, Moral and Info parameters are presented in table \@ref(tab:powers) below.

```{r powers, results = "asis", cache=TRUE}
kbl(tab_powers, col.names = c("# Sim", rep(c("Effect", "Power"), 3)),
    digits = 4, booktabs = TRUE,
    caption = "Estimated power for incremental effects.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE) %>% 
  add_header_above(c(" " = 1, "Threat" = 2, "Moral" = 2, "Info" = 2))
```

[^2]: Due to equivariance property of ordinary least squares the power of a test associated with a parameter, for a given sample size, depends only on the value of that parameter and not on the values of all other regressors.

Looking at table \@ref(tab:powers) above one can see that the experiment has enough power to reject the null hypothesis only for the Threat effect. Due to the estimated effect size for Moral and Info, we should have a much larger sample to have reasonable power to reject the null hypothesis of zero effect. That is to say even if the Moral and Info effects were significant, this experiment would not be able to point that out because the $MDE$ for these two regressors are larger than the estimated effect.

### Explain how the authors conducted inference.

First we shall notice that the authors conducted a _linear probability model_ -- LPM regression on the binary outcome. Suffice to say that in footnote 18, page 652 there is this explanation for the LPM choice: _"In the following, we estimate equations with many interaction terms as explanatory variables. Computing correct interaction effects in nonlinear models becomes tedious and computationally quite intensive (see @Ai2003). Therefore, and to ease comparability between estimations, we employ the linear probability model throughout the whole paper. The results from Table 2 are basically identical to those from probit estimations, which are available from the authors upon request"_. Therefore, the main reason for a LPM regression is the interaction terms between Threat and the other effects, Moral and Info. But in the case of this homework where we will not conduct the interaction regressions, a binary outcome model could easily be used.

Given the regression was a LPM, the authors conducted their inference relying on asymptotic theory for heteroskedasticity robust standard errors. No clusters were used, since the mailings were individually expedited and it is a reasonable assumption that people who receives a tax authority letter asking them to comply with their duties usually keep this information to themselves. Therefore, a spillover effect can be ruled out to neighbors and other mailing recipients, for example.

Even though the no cluster assumption being reasonable, the authors devote the small section 6.2 to discuss the indirect treatment effects. When controlling for municipalities with high mailing coverage, actually they exclude from the sample individuals living in these areas, they find a slightly stronger effect for Threat and Info. The authors attribute this finding not to spillovers, but instead to possible changes in sample composition.

If spillover effects are expected by some reason, then clustering errors and fixed effects at the cluster level can be introduced in the specification and inference can be done. Unfortunately the authors did not take this path.

### Use the assessment proposed by @Ferman2019 to check whether inference based on the asymptotic distribution of the estimator is reliable.

The proposed assessment is carried out following the algorithm laid out in author's paper:

Consider the model


\begin{equation}
y_{i}=\mathbf{x}_{i} \boldsymbol{\beta}+\epsilon_{i}
(\#eq:simple-model)
\end{equation}


where $y_{i}$ is the outcome, $\mathbf{x}_{i}$ is an $1 \times K$ vector of covariates, and $\boldsymbol{\beta}$ is the parameter of interest. We observe $\left\{y_{i}, \mathbf{x}_{i}\right\}$ for a sample of $i=1, \ldots, N$ observations. Let $\mathbf{y}=\left[y_{1} \ldots y_{N}\right]^{\prime}$, $\mathbf{X}=\left[x_{1} \ldots x_{N}\right]^{\prime},$ and $\boldsymbol{\epsilon}=\left[\epsilon_{1} \ldots \epsilon_{N}\right]^{\prime}$. Let the null hypothesis be given by $\mathbf{R} \boldsymbol{\beta}=\mathbf{q},$ for a $J \times K$ matrix $\mathbf{R}$ and a $J \times 1$ vector $\mathbf{q}$. A step-by-step procedure to calculate the assessment is given by:

+ Step 1: estimate model \@ref(eq:simple-model) imposing the null hypothesis. Let $\widehat{\boldsymbol{\beta}}_{0}=\underset{\mathbf{b} \in \mathbb{R}^{K}: \mathbf{R} \mathbf{b}=\mathbf{q}}{\operatorname{argmin}} \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\mathbf{x}_{i} \mathbf{b}\right)^{2}$

+ Step 2: store the predicted values from the restricted regression in Step 1, $\mathbf{X} \widehat{\boldsymbol{\beta}}_{0}$

+ Step 3: do $\mathcal{B}$ iterations of this step. In each step:

  - Step 3.1: draw a random vector $\boldsymbol{\epsilon}^{b}$ from a chosen distribution, and put it in place
of the residuals from Step $1 .^{10}$ We generate $\mathbf{y}^{b}=\mathbf{X} \widehat{\boldsymbol{\beta}}_{0}+\boldsymbol{\epsilon}^{b}$

  - Step 3.2: estimate the unrestricted model with $\mathbf{y}^{b}$ instead of $\mathbf{y}$

  - Step 3.3: test the null hypothesis using the inference method that is being assessed for a significance level of $\alpha \% .$ Store whether the null is rejected in this draw.

+ Step 4: the assessment for this inference method is given by the proportion of the $\mathcal{B}$ simulations in which the null is rejected.

We perform this assessment for each parameter in equation \@ref(eq:reg-full) considering as null hypothesis that _all coefficients_ are zero. The assessment values for each parameter is presented in table \@ref(tab:assess) below.

```{r assess, results = "asis"}
kbl(tab_assess, 
    col.names = c("Sig. level", "Mailing", "Threat", "Moral", "Info"),
    digits = 4, booktabs = TRUE,
    caption = "Assessment on parameters for Mailing, Threat, Moral and Info.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE) 
```

According to the above table, it seems the asymptotic inference is not problematic, although, as the author alerts, _"if the assessment is close to $\alpha$%, then this would not provide a definite indication that the inference method is reliable"_. Hence, it is still necessary to provide good argumentation on why the asymptotic inference being done is reliable. 

### Calculate the p-values using randomization inference. How does that differ from the p-value based on the asymptotic distribution?

I have chosen as statistic the absolute difference in means. When testing the "mailing" effect the mean response for non-recipients is compared to the mean response from individuals who received the letter. The absolute value of this difference determines the statistics' base line. Then one thousand permutations are carried, shuffling the observations of treatment but not responses, since the sharp null hypothesis is no effect whatsoever. For each permutation, the same statistic is computed and then compared to the base line. The proportion of permutation statistics which are greater in magnitude than the base line is reported in table \@ref(tab:ppvals). That is, Fisher's exact p-values.

For incremental treatments, threat, moral and info, the difference of means is taken in relation to mailing, since we are interested in the partial effects over the one already captured by mailing.

```{r ppvals, results = "asis"}
kbl(ppvals_df, digits = 4, booktabs = TRUE, 
    col.names = c("Treatment", "Exact p-value"),
    caption = "Randomization inference, p-values from 1000 permutations.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE)
```

One can see from the above table and comparing to the results obtained from full regression, the p-value for mailing is indeed very close to zero, this treatment being highly significant. Moral and Info treatments, although with different p-values, continue not being significant. The remarkable change is the p-value for the threat. From a very low p-value, being significant at 1% when asymptotic inference is carried out, the randomization inference attributes an exact p-value of `r format(ppvals_df[2,2], digits = 2)`, and now one can only reject the null hypothesis for this treatment at 10% level of significance.

### Discuss potential problems in the experiment. Discuss how you would be able to check whether these problems are relevant (for example, by looking at the available data or by collecting more data), or how the experiment could have been done differently to avoid such problems. Be clear about whether the potential problems you raised were discussed in the paper.

One problem I can see in the experiment is the fact the population is actually a _selected_ list of potential evaders. The authors make it clear in the paper, but it lacks some discussion about how this inherently carries the selection bias from GIS (i.e. the tax authority). So, although the experiment is completely randomized within GIS' list, it is not representative of the whole population, not even the actual population of evaders. Therefore, the results achieved, the estimated treatment effects, refer to "potential evaders as thought by the tax authority" and may suffer from external validity. The results are tightly linked to that specific list of potential evaders, which clearly brings some sort of selection bias together with it. May the authors, or any other researcher, try to extrapolate the results to other list of potential evaders (e.g. even in Austria, but different period) these findings may not hold up. In essence, what has been estimated was the average treatment effect, _given the subject is on the list_.

A full randomization on the whole Austrian population could deal with this external validity problem, making it clear the sample where the control and treatment groups are chosen from is representative. In fact, in this case the experiment would not face sampling uncertainty. This design may be very well feasible, since it is reasonable to think the tax authority, or any other government agency, has records of all households in the country. Even if we allow for a random sample from the records of the whole population we mitigate the problem of external validity, although introducing sampling uncertainty in this case. The main issue not to do this randomization is to avoid sending mailings to people that clearly are complying with their duties, which the authors point in their paper that may have a negative effect on the behavior of the wrongly targeted good tax payers.

The experiment seems to have enough point observations such that it is enough powered to carry asymptotic inference, thus data collection is not an issue, **unless**, a cluster based inference is to be made. In this case the asymptotic inference is valid only when both number of clusters in control and treatment groups tends toward infinity. 

The authors comment very briefly in section 6.2 about indirect treatment effects and possible control for municipalities with high levels of evasion. If had they specified a cluster based design, clusters being municipalities, and fixed effects at cluster level to capture the informal social norm that may be present, the number of clusters could be an issue. 


### Replication bonus

In this section I will replicate the results from tables 2 and 3 from @Fellner2013. The authors conducted their regression estimations in Stata, while I coded everything in R, thus it is important to show we can arrive at the same results in both languages.

```{r table1-fellner, results = 'asis', warning=FALSE}
kbl(tab1, digits = 2, booktabs = TRUE,     
    col.names = c("", "", "Gender", "Age",
                  "Population", "Pop. density", "Compliance",
                  "Observations"),
    caption = "Replication of Table 1. Mean values of individual and municipality characteristics per treatment.") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_classic(full_width = FALSE)
```

```{r table2-fellner, results = "asis", warning=FALSE}
fixest::setFixest_dict(c(resp_A = "Registration", 
                         resp_B = "Contract update", 
                         resp_all = "Overall response",
                         mailing = "Mailing",
                         threat = "Threat",
                         appeal = "Moral",
                         info = "Info",
                         i_tinf = "Threat x Info",
                         i_tapp = "Threat x Moral",
                         threat_evasion_D1 = "Threat x Evasion",
                         appeal_evasion_D1 = "Moral x Evasion",
                         info_evasion_D1 = "Info x Evasion",
                         evasion_1 = "Evasion",
                         threat_evasion_D2 = "Threat x Evasion",
                         appeal_evasion_D2 = "Moral x Evasion",
                         info_evasion_D2 = "Info x Evasion",
                         evasion_2 = "Evasion",
                         "(Intercept)" = "Constant"))
# For now, only if output is PDF
if (tex_output) {
  fixest::etable(reg_21, reg_22, reg_23, reg_24, reg_25, reg_26,
                 tex = tex_output,
                 se = "White",
                 digits = 3,
                 fitstat = "",
                 order = c("Mailing", "^Threat$", "^Moral$", "^Info$", "Threat x Moral",
                           "Threat x Info", "Constant"),
                 title = "Replication of Table 2.")
}
```

```{r table3-fellner, results = "asis"}
if (tex_output) {
  fixest::etable(reg_31, reg_32,
                 tex = tex_output,
                 se = "White", 
                 digits = 3,
                 fitstat = "",
                 keep = c("Mailing", "Threat", "Moral", "Info", "Evasion"),
                 title = "Replication of Table 3.")
}
```

\newpage
### Annex - R Code

```{r annex, code=readLines("replication_fellner.R"), eval = FALSE, echo=TRUE}

```


## References