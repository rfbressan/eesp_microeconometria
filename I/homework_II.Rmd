---
title: "Microeconometrics I"
subtitle: "Homework II"
author: 
  - "Professor: Andr√© Portela"
  - "Student: Rafael F. Bressan"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    number_sections: false
    highlight: default
  bookdown::pdf_document2:
    number_sections: false
    highlight: default
    # keep_tex: true
    toc: false
bibliography: references.bib
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
tex_output <- knitr::is_latex_output()
stg_type <- ifelse(tex_output, 'latex', 'html')

library(dplyr)
library(tidyr)
library(ggplot2)
library(kableExtra)
library(stargazer)
library(texreg)
load("input/homework_II.RData")
```

You have been provided with a sample of 12,834 individuals in the labor force extracted from the 2019 annual supplement of the 2019 US Current Population Survey. Your goal is to estimate the causal effect of union membership/coverage (variable `union`) on weekly earnings (variable `earnings`). The dataset contains many other variables, which could potentially be used as controls (check the dataset dictionary).

1. **As a starting point, compare average earnings among individuals with union coverage (`union==1`) vs individuals without such coverage (`union==0`). What is the estimated difference? Is it statistically significant? Do you think such a difference is a credible estimate for the causal impact of union coverage? Why?**

First we notice there is an imbalance on the number of unionized and not unionized workers, the former being `r length(earnings_union)` workers while the last is much larger, `r length(earnings_not)`[^1]. The difference in average earnings is \$`r format(avg_dif, digits = 2)`. It is statistically significant by a t-test of difference in sample means, with different variances. The t-statistic is `r format(test$statistic, digits = 2)`.

Since `union` is a dummy variable, this difference in means can be thought as a simple regression of `earnings` on `union` and the coefficient $\beta_1$ is the difference in means.

$$
\begin{equation}
\text {earnings}_{i}=\beta_{0}+\beta_{1} \text{union}_{i}+\epsilon_{i}
(\#eq:earnings)
\end{equation}
$$

This is clearly not a good estimator for the causal effect of union coverage on wages due to **selection bias**. There may be many hidden factors driving wages that are also correlated to the willingness of being part of a union, thus, the difference in average wage of unionized workers and not unionized workers is a _biased_ estimate for the causal effect under study. 

From now on, let's adopt the notation settled in @Imbens2015 and let the individual observations be indexed by $i \in \{1, \ldots, N\}$. The potential outcomes of individual $i$ are represented by $Y_i(0)$ if no treatment is taken and $Y_i(1)$ if the individual has been treated. Comparisons of $Y_i(1)$ and $Y_i(0)$ are unit-level causal effects, where we adopt the additive form, that is, the **individual causal effect** is defined as $Y_i(1) -Y_i(0)$. The response we can observe from an individual is $Y_i=Y_i(1)W_i+Y_i(0)(1-W_i)$ for a treatment $W_i \in \{0, 1\}$. The **average treatment effect** - ATE, is $ATE=E[Y_i(1)-Y_i(0)]$, while the average treatment effect on the treated - ATT is $ATT=E[Y_i(1)-Y_i(0)|W_i=1]$.

Thus, when we just compare the average earnings of workers, unionized and not, we are making the following estimation:

$$
\begin{align*}
E[Y_i(1)|W_i=1]-E[Y_i(0)|W_i=0]&=\underbrace{E[Y_i(1)|W_i=1]-E[Y_i(0)|W_i=1]}_{ATT}\\
&+\underbrace{E[Y_i(0)|W_i=1]-E[Y_i(0)|W_i=0]}_{\text{selection bias}}
\end{align*}
$$
and while the ATT is a valid estimation of causal effect, we don't have a clean figure of it when taking a simple difference of means, which will be plagued by selection bias.

[^1]: After removing observations where there is no earnings information available, `r ear_na` individuals.

2. **In order to improve and/or assess the credibility of your previous results, you decide to run a linear regression:**

$$
\begin{equation}
\text {earnings}_{i}=\beta_{0}+\beta_{1} \text {union}_{i}+\gamma^{\prime} Z_{i}+\epsilon_{i}
(\#eq:earnings-cov)
\end{equation}
$$
where $Z_{i}$ are a set of controls.

a. Specify a linear model (\@ref(eq:earnings-cov)) by laying out a set of covariates $Z$ to be included as controls. Justify your set of controls. What is the interpretation of $\beta_{1}$ in your model?

In case one wants to include covariates to the model, it's appropriate to make sure these variables help to explain variations in earnings, be that by theoretical modeling or empirical findings on earnings. With that qualification in mind, I have chosen the following variables to be in the covariates' set. Notice that nothing is being said about the covariates having a _causal effect_ on earnings, but merely these variables help to explain variations in earnings, that is, they are correlated. Table \@ref(tab:covariates) bellow presents the variables chosen and a brief description based on the dictionary provided.

```{r covariates, results = "asis"}
kbl(covariates, booktabs = TRUE,
    col.names = c("Variable", "Description"),
    caption = "Covariates chosen to model (2).") %>% 
  kable_classic(full_width = FALSE)
```

Those variables were chosen based on previous literature relating earnings to social and economic factors, [@Ashenfelter2010]. Age is usually included in earnings regressions as a quadratic polynomial, reflecting the fact that there is an "optimal" age for earnings (or wages). Other variables like `female`, `race` and `veteran` follows from a literature that depicts prejudice or unfairness when setting wages. Education is a classic regressor for earnings, since it is believed the more educated a worker is, higher her productivity, and by a neoclassical argument, wages must reflect the marginal productivity of labor. The `class_of_worker` is included to capture some specificities from the demand side of labor, for example, due to imperfect competition on the final product markets, some firms may be more profitable than others, and part of this profitability is shared, through a Nash bargain, with its employees.

I have chosen not to include variables like `worked_last_year` and `class_of_worker_last_year` because, while they may be related to current earnings, these are essentially variables that capture model dynamics. Since I am more interested in cross-sectional results, lagged variables would have a different interpretation and would capture dynamic factors, like persistence in earnings, but not economic or social traits. 

Adding covariates to our model helps in making the case for a causal interpretation of $\beta_1$, but it is debatable whether we are including all relevant variables or not. A causal interpretation is due **only** if we control for all factors that affect earnings and are correlated to the worker's choice of entering the union. This is _unlikely_ to be true in the current setup with a limited number of variables, thus, it is still uncertain one can make causal inference with model \@ref(eq:earnings-cov).

THEORY OF SELECTION ON OBSERVABLES HERE
HOW CONTROLING FOR ALL COVARIATES MAKE CAUSAL INTERPRETATION

b. Estimate the specified model. What is your estimate of $\beta_{1}$? Is it significant? Briefly comment on your results.

Once chosen the covariates, we have a dataset of one dependent variable, `earnings` and eight regressors, the treatment `union` and seven selected controls from table \@ref(tab:covariates). Before attempting to make a regression from this data, we must first make sure we don't have any missing observations. The first step is to exclude from our dataset, any row where `earnings` is missing, there were 19 such rows. Next we investigate further if any other regressors don't have observations. Table \@ref(tab:missings) shows the resutls.

```{r missings, resutls='asis'}
kbl(data.frame(missings), col.names = "Missings", booktabs = TRUE,
    caption = "Missing data for model (2).") %>% 
  kable_classic(full_width = FALSE)
```

There are 83 missing values for veteran status. Since it is much more likely not to be a veteran[^2], the choice to impute zero as the value for the missings is appropriate. The option would be to discard such observations, but I'd rather preserve observations that otherwise are complete.

[^2]: The ratio of veterans to non-veterans in this dataset is `r format(vet_ratio[2]/vet_ratio[1], digits = 2)`. Also, we will not elaborate on a method to impute missing values, like the probability of being a veteran given a set explanatory variables so, we set every missing to zero.

Now that we have imputed values for missing veteran status, we shall notice that most of our included variables are categorical. Except for `age`, which is an integer number, all other regressors are categorized according to numerical codes, but should not be interpreted as ordered data. For example, code 20 versus code 28 in `class_of_worker` should not be taken as 20 is lower than 28 as they are just codes for classification of different types of employment. Hence, our approach is to set dummy variables for each level such a categorical variable may take. When doing this categorization, one must be careful to have enough observations in any such level. Table \@ref(tab:nobs-level) shows the results of this categorization.

```{r nobs-level, resutls='asis'}
kbl(nobs_level, booktabs = TRUE, 
    caption = "Regressors categorization.",
    row.names = FALSE,
    format.args = list(na.encode = FALSE)) %>% 
  kable_classic(full_width = FALSE)
```

With a cleaned, prepared and meaningful dataset we are now able to perform the regression in model \@ref(eq:earnings-cov). The coefficient found on variable `union` is the impact of being unionized on our _baseline_ individual, that is, the individual in the first line of table \@ref(tab:nobs-level), meaning a not unionized, male, white, married with spouse present and so on. If we are interested in the effect of joining a union for other representative individuals, we should include the interaction of union to all other covariates and analyze the result.

```{r regression1, results='asis'}
stargazer(model0, model1, type = stg_type,
          se = list(robust0.se, robust1.se),
          p = list(robust0.pval, robust1.pval),
          #column.labels = c("(1)"),
          dep.var.labels = "earnings",
          title = "Results from regressions.",
          label = "tab:regression1",
          keep = "union",
          notes = "White corrected standard errors." 
)
```

From the results of table \@ref(tab:regression1) we find that $\beta_1$ for the estimated model is significant and has a value of `r format(model1_coef_tbl["union", "Estimate"], digits = 2)`, which is lower than the simple difference of means found in model (\@ref(eq:earnings)). We are still reluctant to give a causal interpretation to the coefficient $\beta_1$ in model (\@ref(eq:earnings-cov)) since it is not at all clear we have controlled for every factor that affects earnings and are correlated to the choice of joining a labor union.

Using the Frisch-Waugh-Lovell theorem, we can show that the OLS estimator for $\beta_{1}$ has the representation:

$$
\begin{equation}
\hat{\beta}_{1}=\sum_{i=1}^{N} \text {union}_{i} \cdot \omega_{i} \cdot \text{earnings}_{i}-\sum_{i=1}^{N}\left(1-\text{union}_{i}\right) \cdot \omega_{i} \cdot \text{earnings}_{i}
(\#eq:beta1hat)
\end{equation}
$$
where weights $\omega_{i}$ are:

$$
\begin{equation}
\omega_{i}=\frac{\text{union}_{i}+\hat{\xi}_{i}\left(1-2 \text {union}_{i}\right)}{\text{SSR}_{\text{union}, Z}}
(\#eq:weights)
\end{equation}
$$

with $\hat{\xi}_{i}$ being the residual of observation $i$ from a linear regression of union $_{i}$ on $Z,$ including an intercept; and SSR union, $Z$ is the sum of squared residuals of this auxiliary regression.

c. compute the weights for your specification using the formula above. Report summary statistics for the distribution of weights in the control and treatment groups. Do the weights sum to one in the control group? What about the treatment group? Are there any negative values? What about outliers? How do these weights compare with those from other estimators you have seen in class (e.g. Horvitz-Thompson)? Why? Hint: Section III of Imbens G. Matching Methods in Practice. Journal of Human Resources, 2015 ; 50(2): 373-419

Table \@ref(tab:w-summary) presents summary statistics for weights by control and treatment groups, not in union and unionized respectively.

```{r w-summary, results='asis'}
kbl(w_summary, booktabs = TRUE, digits = 6,
    caption = "Summary statistics for weights.",
    col.names = c("Statistic", "Not Union", "Union")) %>% 
  kable_classic(full_width = FALSE)
```

The sum of weights for the control group is -1, while that for treatment group is approximately 0.277. There are negative values for the weights in both groups, but the values are very small in magnitude. As for outliers, we better make a boxplot, shown in figure \@ref(fig:weights-box).

```{r weights-box, fig.cap="Box-plot of weights.", fig.align='center'}
ggplot(data_cov, aes(factor(union), weight)) +
  geom_boxplot() +
  labs(x = "Union") +
  theme_classic()
```

3. **State a causal estimand of interest (ATT or ATE) and the assumptions required for the identification of this effect on a selection-on-observables framework. Explain why you require these assumptions.**

4. **Report balance checks (t-stats and normalized differences) for a priori relevant (for identification) covariates in the treatment and control group. Are these covariates balanced between groups?**

5. **Estimate a propensity score model for union using logistic regression. State the variable selection method you will use (e.g. "I'll use Imbens and Rubin's stepwise selection algorithm, taking ... as base variables, and letting their method select $\ldots$ "). Comment on your results. What is the normalized difference of the latent indices of the logistic model, $X_{i}^{\prime} \hat{\kappa},$ in the treatment vs control group?**

6. **Assess the quality of your estimated propensity score by verifying its balancing properties (e.g. dividing dataset in blocks using Imbens and Rubin's approach and verifying covariate balance within each block).**

7. **Use Imbens and Rubin's approach (Chapter 16) to trim your dataset in order to improve overlap. Rereport the results in (4). What happened to them? What about the normalized difference of the latent indices of the logistic model?**

8. **Estimate your causal estimand of interest using subclassification on the estimated propensity score.**

9. **Estimate your causal estimand of interest using matching on the estimated propensity score.**

10. **Estimate your causal estimand of interest using inverse probability weighting (Horvitz-Thompson). Briefly compare the results from your different estimators.**

\newpage
### Annex - R Code

```{r annex, code=readLines("homework_II.R"), eval = FALSE, echo=TRUE}

```


## References