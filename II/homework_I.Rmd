---
title: "Microeconometrics II"
author:
- 'Professor: André Portela'
- 'Student: Rafael F. Bressan'
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: no
    highlight: default
    toc: no
  bookdown::html_document2:
    number_sections: no
    highlight: default
  # pdf_document:
  #   toc: no
  # html_document:
  #   toc: no
  #   df_print: paged
subtitle: Homework I
bibliography: references.bib
---


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# tex_output <- knitr::is_latex_output()
# stg_type <- ifelse(tex_output, 'latex', 'html')

# library(data.table)
library(tidyverse)
library(kableExtra)
library(AER)
library(modelsummary)
library(sandwich)
library(lmtest)
library(rdrobust)
library(rddensity)
# library(grf)
# library(stargazer)
# library(texreg)
load("input/homework_I.RData")
```

# Part 1: Instrumental variables

You have been provided with a sample of first-born children aged between 10 and 18 years old living with both of their parents. The sample was constructed from Census 2010 . Each entry in the dataset corresponds to a first-born child from both spouses. There can be multiple first-born children in a given family/couple if the first birth from both spouses was a multiple birth. A detailed data dictionary can be found in `dicionario.xlsx`. Your goal is to estimate the causal impact of the number of children in the family (variable `family_number_children`) on the years of education of the couple's first-born child(ren) (variable `first_child_years_of_education`)

**Question 1** In this item, we will explore the identifying power of the monotone treatment response (MTR) and monotone treatment selection (MTS) assumptions.

_(a) State the MTR and MTS assumptions - **in the most plausible direction** - in this context. Does economic theory have any predictions on the validity of these assumptions? Hint: See the discussion in @ponczek2012new ._

Both MTR and MTS definitions are made in @Manski2000 , they respectively are:

_MTR: Let $T$ be an ordered set. For each $j \in J$,_

$w_2\geq w_1\implies y_j(w_2)\geq y_j(w_1).$

_MTS: Let $T$ be an ordered set. For each $w \in W$,_

$u_2\geq u_1\implies E[y(w)|z=u_2]\geq E[y(w)|z=u_1].$

where $W$ denotes the treatment status, $y_j$ is the potential outcome for person $j \in J$.

In this context, the MTR assumption is most plausible when the number of children in the family **reduces** years of education of the first born child. That is:


\begin{equation}
  \text{MTR: }w_2\geq w_1\implies y_j(w_2)\leq y_j(w_1)
  (\#eq:mtr)
\end{equation}

considering that $w_i$ is the number of children in the family and the outcome $y_j$ is years of education of the first born.

In a similar way, the MTS assumption in this context will state that potential outcomes for first born children in bigger families are **lower** than the ones in smaller families.


\begin{equation}
  \text{MTS: }w_2\geq w_1\implies E[y(w)|W=w_2]\leq E[y(w)|W=w_1]
  (\#eq:mts)
\end{equation}

Economic theory, as presented in @ponczek2012new Introduction section, predicts that larger families, with a higher number of children will have to share its scarce resources among all children in household, thus leaving the first born with a lower level of education when compared to smaller families. Although, the empirical evidence is mixed, many of the literature for developed economies show no effect of family size on child quality while for developing countries, the negative hypothesized effect is found.

_(b) Report a table with average years of schooling by number of children, as well as the frequency of each value of variable "number of children" in the sample. You may want to use individual sample weights (variable person_weight) in estimation in order to better account for the population of interest._

The total population represented by this sample is `r format(sum(educ_tab$frequency), big.mark=",")`, considering a person's sample weight.

```{r 1-b, results='asis'}
kbl(educ_tab, digits = 2, booktabs = TRUE,
    caption = "Years of schooling and number of children by family size.",
    col.names = c("Children", "Years Educ.", "Effective Obs.")) %>% 
  kable_classic(full_width = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position"))
```


_(c) Compute the upper and lower bounds on the ATE of increasing the number of children in the family from 1 to 2,2 to 3,3 to 4,4 to 5,5 to 6 etc. Compute 95% confidence intervals to these bounds using the bootstrap. You should draw samples of households with replacement in order to properly account for the sampling process. You may want to set the probability of sampling a household proportional to the household weight (variable `household_weight`) in order to better replicate the sampling process._

First we will derive some results **for binary** treatment, following the MTR and MTS specifications from equations \@ref(eq:mtr) and \@ref(eq:mts) to get an intuition of what changes from the usual MTR and MTS assumptions. When we impose the MTR assumption in \@ref(eq:mtr), we get a zero **upper bound** on ATE, while the MTS assumption yields a **lower bound** that is equal to the naive difference of means between the two groups. We see that upper and lower bound were interchanged in this new specification. Putting the two assumptions together we have that:


\begin{equation}
  E[y_j|W=\bar w_{n}]-E[y_j|W=w_{n}]\leq\text{ATE}\leq 0
\end{equation}

where the treatment $\bar w_{n}$ refers to a family with **more** than $n$ children, and $w_{n}$ is a family with $n$ children or less, such that $P(W=\bar w_n)=\pi$ and $P(W=w_n)=1-\pi$.

We begin with the proof that MTR assumption implies the zero upper bound.

```{proposition, mtr, echo = TRUE}
Given the MTR assumption on \@ref(eq:mtr) the upper bound -- UB -- on the average treatment effect -- ATE -- is zero.
```

```{proof, mtr-proof, echo = TRUE}
Suppose the treatment is binary with two levels, $w_2\geq w_1$. The probability of being assigned to $w_2$ is $\pi$. Then the MTR assumption in \@ref(eq:mtr) implies that $y_j(w_2)\leq y_j(w_1)$, and we have the following two inequalities,

$E[y_j(w_2)|w_1]\leq E[y_j|w_1]$ and $E[y_j|w_2]\leq E[y_j(w_1)|w_2]$

The ATE has the following observational-counterfactual decomposition,

$E[y_j(w_2)-y_j(w1)]=\pi E[y_j|w_2]-(1-\pi)E[y_j|w_1]-\pi E[y_j(w_1)|w_2]+(1-\pi)E[y_j(w_2)|w_1]$
  
Making use of the inequalities in the observational-counterfactual decomposition to obtain an upper bound for the ATE we have that:
  
  \begin{align*}
    \text{ATE}=E[y_j(w_2)-y_j(w1)]&\leq\pi E[y_j|w_2]-(1-\pi)E[y_j|w_1]-\pi E[y_j|w_2]+(1-\pi)E[y_j|w_1]\\
    &=0
  \end{align*}
```

Now we prove that the MTS assumption implies a lower bound on the ATE equal to $E[y_j|w_2]-E[y_j|w_1]$.

```{proposition, mts, echo = TRUE}
Given the MTS assumption on \@ref(eq:mts) the lower bound -- LB -- on the average treatment effect is equal to the difference of means between the treatment groups, $\text{ATE}\geq E[y_j|w_2]-E[y_j|w_1]$.
```

```{proof, mts-proof, echo = TRUE}
Suppose once again the treatment is binary with two levels, $w_2\geq w_1$. The probability of being assigned to $w_2$ is $\pi$. Then the MTS assumption in \@ref(eq:mts) implies that potential outcomes for treatment group at $W=w_2$ are lower than in group with $W=w_1$, and we have the two inequalities,

$E[y_j(w_1)|w_2]\leq E[y_j|w_1]$ and $E[y_j|w_2]\leq E[y_j(w_2)|w_1]$
  
by the observational-counterfactual decomposition the ATE has a lower bound given by:
  
  \begin{align*}
    \text{ATE}=E[y_j(w_2)-y_j(w1)]&\geq\pi E[y_j|w_2]-(1-\pi)E[y_j|w_1]-\pi E[y_j|w_1]+(1-\pi)E[y_j|w_2]\\
    &=E[y_j|w_2]-E[y_j|w_1]
  \end{align*}
```

Therefore, for a multi-level treatment we can expect the same phenomenon to occur, the upper and lower bound computations will be flipped. This in turns defines that our **upper bound** on ATE will be zero, while the lower bound can be computed by eq. 9.19 from @Manski2009.

\begin{equation}
\Delta(s,t)\leq \sum_{t^\prime>t}E(y|w=t^\prime)P(w=t^\prime)+E(y|w=t)P(w\leq t)-\sum_{s^\prime < s}E(y|w=s^\prime)P(w=s^\prime)-E(y|w=s)P(w\geq s)
(\#eq:lower-bound)
\end{equation}

Below we present a table with empirical mean of years of study and the distribution number of children across families, a la Table I in @Manski2000.

```{r manski-tbl, resutls='asis'}
kbl(manski_tbl1, digits = 4, booktabs = TRUE,
    caption = "Mean of years of study by number of children",
    col.names = c("w", "E[y|w]", "P(w)", "Size")) %>% 
  kable_classic(full_width = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position"))
```

In table \@ref(tab:lower-bound) we have the estimated lower bound for the causal effect and the 2.5% and 97.5% bootstrap quantiles, as in @Manski2000 Table II.

```{r lower-bound, results='asis'}
kbl(manski_tbl2, digits = 4, booktabs = TRUE,
    caption = "Lower bound on years of study for first child.",
    col.names = c("s", "t", "Estimate", "2.5% quant.", "97.5% quant.")) %>% 
  kable_classic(full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Lower Bound on Delta(s,t)" = 3)) %>% 
  kable_styling(latex_options = c("HOLD_position"))
```

As a complementary analysis of our resampling method, we present below the histograms of bootstrapped lower bounds by treatment level $t$.

```{r lb-hist, fig.cap="Distribution of bootstrapped lower bounds"}
lb_hist
```

**Question 2** In this item, we will use an instrumental-variable approach in estimating the causal impact of the number of children on years of schooling of the first child. We will follow the approach in @ponczek2012new, whereby we first restrict our sample to families with two or more births from the couple (variable `family_number_births` $\geq 2$ ). We then propose to instrument the number of children with `second_birth_ismultiplebirth`, which indicates whether the second birth of the couple was a multiple birth [3]

_(a) Make the sample restrictions previously discussed. How many second births are multiple births?_

When considering only the families with more than one birth we end up with `r n_two_plus_births` observations and the number of second births that are multiple is **`r n_second_multiple`**.

_(a) Under which assumptions does an instrumental variable approach identify a treatment effect in our setting? What treatment effect? Do these assumptions seem plausible to you? Why? Looking at the dataset, do you think any controls should be included? Why?_

First we need the usual IV assumptions of relevance and exclusion. Then, for a causal interpretation of the estimated parameter we need that the instrument must be as good as randomly assigned after controlling for relevant variables. Suppose we have an instrument $Z$ for treatment $W$, with $Z_i, W_i \in \{0, 1\}$ then:

i) Relevance: $E[Z_i W_i]\neq 0$

ii) Exclusion: $E[Z_i \varepsilon_i]=0$

iii) As good as randomly assigned: $Y_i(1), Y_i(0)\perp Z_i|\mathbf{X_i}$

In the present case, the instrument variable of choice is a multiple birth for the second born child. Relevance is satisfied since a multiple birth is clearly related to the family size. The exclusion restriction of multiple births has been debated in the literature, but many recognized authors have chosen this instrument on the assumption that having twins is largely random, thus, not related to any possible omitted variable that is related to the outcome years of education of first born child. Finally, from the previous discussion about the exclusion restriction, we do believe this instrument is as good as randomly assigned. 

Only if the instrument $Z_i$ were **perfectly** correlated to the the treatment $W_i$ we would be able to identify the ATE. Since this is not usually the case, the instrument is only moderately correlated (i.e. relevant) to the treatment, ATE is not identifiable from this setting. This is the case of imperfect compliance, and in order to identify some other effect from the data we need a fourth assumption:

iv) Monotonicity: $W_i(1)\geq W_i(0)$

That is, the effect of the instrument on the treatment points toward the same direction for every unit. If we assume monotonicity, then the Local Average Treatment Effect -- LATE -- is identified. The LATE has the meaning of the average treatment effect on the subpopulation of **compliers** only. The compliers are those individuals who would take the treatment if induced to do so (by the instrument variable), but otherwise would refrain.

Actually, the twins instrument is special in the sense it does not allow a subpopulation called **never-takers**. If a multiple second birth is present, there is no way this family ends up with only two children, it has to be three or more. In this special case, LATE coincides with the average treatment effect on the untreated -- ATU, see @Angrist2008, section 4.4.2 for a discussion.

Therefore, for the instrumental variable `second_birth_ismultiplebirth` we do believe all assumptions are plausible and the LATE can be estimated from data.

The set of control variables could potentially be empty, since our instrument is as good as randomly assigned. If the researcher chooses to control for some characteristics, it is more related to improve the estimate precision than satisfying assumption iii) above.

We do not necessarily think any control **should** be include, although, if we can control for other characteristics that helps in determine the outcome, this inclusion would improve our estimate by reducing the estimator's variance. Such variables could be, age of the first child, whether the first child is a girl. The indicative of first birth is multiple is also a good candidate for control variable, since it may explain the schooling of the multiple first born.


_(b) Estimate the treatment effect using 2SLS. Include any covariates you regarded as necessary in the previous item. Cluster your standard errors at the household level (you may also want to weight observations by the person weight). Is the instrument relevant? Why? Comment on your results._

We estimate two models. The first one is a single regression model of years of education on number of children, instrumented by multiple second birth. The second model adds the covariates child age, girl and multiple first birth.

```{r 2-b, results='asis'}
modelsummary(list(iv_est0, iv_est1),
             statistic_override = list(cl_vcov0, cl_vcov1),
             gof_omit = ".*",
             notes = c("Note: Cluster robust standard errors reported in parentheses. Cluster variable is 'id_household'."),
             output = "kableExtra") %>% 
  kable_classic(full_width = FALSE)
# stargazer(iv_est0, iv_est1,
#           title = "Instrumental variable estimation of treatment effect.",
#           se = list(sqrt(diag(cl_vcov0)), sqrt(diag(cl_vcov1))))
# summary(iv_est0, diagnostics = TRUE, vcov. = vcovCL, cluster = id_household)
```

In order to assess the instrument's relevance we report the first stage **F statistic**. For model 1 and model 2 this statistic is, respectively, `r format(f_test0, digits=2)` and `r format(f_test1, digits=2)`, showing that the chosen instrument is relevant.

The inclusion of control variables didn't change significantly the parameter estimate, as expected. Since our instrument is valid, relevant and randomly assigned, controlling for other relevant variables just improves precision, but do not alter the point estimate. We notice however, that all three control variables are significant, but robust standard error for model 2 improved only slightly.

_(c) Conduct a test for weak instruments. Are your instruments weak? In what sense? Hint: See Section 4 in @andrews2019weak ._

We conduct the Olea and Pflueger (2013) test for weak instrument. The endogenous variable is `family_number_children` and the instrument is `second_birth_ismultiplebirth`, while we add three control variables, `first_child_age`, `first_child_is_girl`, `first_birth_ismultiplebirth`. The reported effective F-statistic was `r weak_iv_test$eff_F`. Since the authors suggested rule of thumb for a 5% test that the worst-case relative bias of 2SLS exceeds 10% is 23.1 for their corrected F-statistic, we **do reject** the null hypothesis that `second_birth_ismultiplebirth` is a weak instrument for `family_number_children`. 

This test uses the first definition of Stock and Yogo of a weak instrument, that is, a instrument is said to be weak when the worst-case bias of two-stage least squares exceeds 10% of the worst-case bias of OLS.

_(d) Report Anderson-Rubin confidence intervals. How do they compare to (b)?_

We run the AR confidence interval for model 2, with control variables. The confidence interval found is `r ar_ci_txt`, while this interval for the same model in item b) was `r iv_ci_txt`, very close to each other.

_(e) Compare your results with the estimates found in Question 1._

We can see that the confidence interval found using the Anderson-Rubin method is tighter than the bounds found in item 1 c) using Manski's approach, in general. Although, for smaller number of children in the family, the lower bound in table \@ref(tab:lower-bound) are tighter than the confidence interval just found. 

# Part 2: Regression discontinuity design

For this part of the list, you have been provided with data from @amarante2016cash, who studies the effect of a cash-transfer program in Uruguay on health outcomes at birth. According to the authors, "the Uruguayan Plan de Atención Nacional a la Emergencia Social (PANES) was a temporary social assistance program targeted to the poorest 10 percent of households in the country, implemented between April 2005 and December 2007 ." Eligibility was defined via a baseline survey conducted with applicants. A probit model for the likelihood of falling below a critical per capita income level was estimated using baseline data, and households whose predicted probability exceeded some threshold were eligible to the program. However, due to imperfect enforcement of the rules of the program, some noneligible mothers did actually receive the cash transfer, whereas some eligible mothers failed to do so.

You have been provided with a dataset where each entry corresponds to a pair (birth,mother) during the program duration. The treatment indicator variable is treat. The eligibility dummy is `eligible` $=\mathbf{1}\{\text{running} > 0\}$, where running is the predicted probability of falling to poverty, already subtracted of the threshold for program eligibility. File `dic_amarante.pdf` contains the description of additional variables in the dataset.

_1. State the assumptions required for the identification of a treatment effect using the discontinuity described above. What do these assumptions mean in this context? What is the interpretation of the treatment effect identified under these assumptions?_

The discontinuity just described is a fuzzy one, the probability of receiving the cash transfer even when the running variable is lower than the specified threshold is above zero while that probability in below one if the individual's running variable is over the threshold. 

Therefore, the key assumptions for the Fuzzy RDD model are:

i) Potential outcomes are continuous in the running variable $x$ at the cutoff value $\bar x$, such that the following limits exist

$$\lim_{x\uparrow \bar x}E[Y_i|x=\bar x] \text{ and } \lim_{x\downarrow \bar x}E[Y_i|x=\bar x]$$
this means that we can compare unities just below the cutoff which had not received treatment and unities above the cutoff which did have been treated, in other words, we can compare the outcomes of compliers.

ii) Potential treatments must be discontinuous at $x=\bar x$. This ensures there is still a discontinuity on the probability of getting treated at $x=\bar x$, although not sharp. Also, this hypothesis is related to the identification of causal effect, since it makes the so called first stage different from zero.

$$\lim_{x'\uparrow \bar x}P(D_i = 1|x=x')\neq \lim_{x'\downarrow \bar x}P(D_i = 1|x=x')$$
This hypothesis is the monotonicity hypothesis without being explicit about the direction of the relation between the running variable and treatment. 

iii) Independence of potential outcomes from treatment status at $x=\bar x$. This is the usual hypothesis of conditional unconfoundedness, but this time we need it to hold at the cutoff value.

$$Y_i(1), Y_i(0) \perp D_i|x=\bar x$$

Since the Fuzzy RDD is similar in nature to an IV approach, we also need two assumptions from instrumental variables, relevance of the running variable and the exclusion restriction, where the running variable does not directly affects the outcome.

Without hypothesis iii) of conditional independence, the treatment effect estimated under a Fuzzy RDD is the Local Average Treatment Effect -- LATE -- which is the effect on compliers, for those with $x=\bar x$. If we impose conditional independence of potential outcomes from treatment at the cutoff value, then the Average Treatment Effect -- ATE -- is recovered[^1].

[^1]: From a preliminary draft of [A Practical Introduction to Regression Discontinuity Designs: Extensions](https://cattaneo.princeton.edu/books/Cattaneo-Idrobo-Titiunik_2018_CUP-Vol2.pdf), p. 88.

2. Report a discontinuity plot between the running variable (x-axis) and program participation (y-axis), as well as a discontinuity plot between the running variable (x-axis) and low birthweight (variable `bajo2500`). What do these plots tell you?

Since we have so many observed units, the scatter plots asked would be clogged with points at $Y = \{0, 1\}$ and would be uninformative. Thus, we opted instead, to cut the running variable into bins and compute y-axis variable's average on a given bin. The point estimate and the $\pm$ one standard error are provided in the following plots.

```{r exp-treat, fig.cap="Probability of getting treated."}
rdplot(panes$treat, panes$running, binselect = 'es', ci = 95,
       kernel = "triangular", 
       x.label = "Running (X)", y.label = "E[T|X]")
```


```{r exp-bajo2500, fig.cap="Probability of having a child weighting 2.5Kg or less at birth."}
rdplot(panes$bajo2500, panes$running, binselect = 'es', ci = 95,
       kernel = "triangular", p = 4,
       x.label = "Running (X)", y.label = "E[Y|X]")
```

Figure \@ref(fig:exp-treat) provides a picture for the discontinuity of the probability of getting treated at $x=0$, although it is not sharp, there is a clear jump on the point estimate at the cuttof value. On the other hand, the probability of having a low weight child does not appear to change at all at the specified cutoff value, as can be seen in Figure \@ref(fig:exp-bajo2500).

3. Estimate the effect of program participation on low birthweight by local linear regression. Precisely state the bandwidth selection method used, the choice of kernel, as well as whether bias correction was employed. What is the first-stage relation, at the cutoff, between program eligibility and program participation? Is it statistically significant? Comment on your second stage results.

Below we present the results (second stage) of our estimations using two kernels, triangular and uniform, for comparison.

```{r rdd, results='asis'}
kbl(est_tab, digits = 4, booktabs = TRUE,
    caption = "RDD estimates. Triangular and Uniform kernels.") %>%
  kable_classic(full_width = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  pack_rows("Uniform", 1, 3) %>% 
  pack_rows("Triangular", 4, 6)
```


Optimal bandwidth was calculated according to the Calonico et al. method as implemented in the R package `rdrobust`. We report bias corrected estimate and variance (Robust), only estimate (Bias-Corrected) and no correction at all (Conventional). 

The second stage results show NO EFFECT of Panes program on the probability of giving birth to a low weight child. The result is robust to kernel choice and bias correction.

The first stage relation is the effect of running variable in the probability of getting the treatment, at the cutoff value. This relation is positive and significant according to our results below. This was expected, since Figure \@ref(fig:exp-treat) showed a relevant positive discontinuity at the cutoff value.

```{r first-stage}
summary(tri_first)
```


4. In order to assess the credibility of your empirical strategy, choose a variable which you may argue is predetermined and estimate the effect of program participation at the cutoff as in the previous item. What do you find?

We have chosen the gestational length in weeks, `semgest` as a placebo variable, since it is mainly determined by biological traits of the mother and the PANES program has no influence on these traits. This is a predetermined variable, the cash-transfer program should have no effect on this outcome and this is indeed the case as shown in results below. 

```{r placebo}
summary(placebo_m1)
```

5. Implement a manipulation test for the running variable in your setting. What do you find?

We implemented the test suggested by @Cattaneo2020 and visually inspecting the plot below we are not able to reject the null hypothesis of no manipulation in the running variable.

```{r manipulation, fig.cap="Manipulation test on PANES program"}
manip_plot$Estplot
```

A summary of this test is provided below.

```{r manip-summ}
summary(manipulation)
```

6. Do you think there are any potential threats to identification and/or estimation in your context? Can you think of any strategies to circumvent these?

In the fuzzy regression discontinuity design, one of the assumptions for identification is inherited from instrumental variable theory, the exclusion restriction. The PANES program was designed with eligibility (i.e. the running variable) based on a probit model estimated from a baseline survey, and it was target to the poorest 10% of households. The exclusion restriction demands that the running variable does not directly impact the outcome, which in our study is child's low weight at birth. Although, we can easily think of being poor (as the running variable is a proxy for it) is directly linked with the child weight at birth, since it is likely the mother does not eat well and enough, specially for the poorest.

This fact would invalidate our running variable as an instrument, and the way to circumvent this would be to choose another variable from the survey to be the instrument. This new variable should attend both the exclusion restriction and be correlated to the probability of being treated (the relevance condition). The later condition can be met by choosing a variable that was significant in the probit model for eligibility.

Also, for the estimation of a RDD, one must always be very careful about the extrapolation needed when utilizing unities that are not exactly at the cutoff value. @Gelman2019 argue that polynomials of order greater than two should not be used in local regressions. High order polynomials are very sensitive to the inclusion of observations and may lead erroneous extrapolation toward the cutoff. We try to avoid this pitfall by using local _linear_ regression and choosing a very tight bandwidth. Also  

\newpage
### Annex - R Code

```{r annex, code=readLines("homework_I.R"), eval = FALSE, echo=TRUE}

```

## References