---
title: "Microeconometrics II"
author:
- 'Professor: Bruno Ferman'
- 'Student: Rafael F. Bressan'
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: no
    highlight: default
    toc: no
  bookdown::html_document2:
    number_sections: no
    highlight: default
  # pdf_document:
  #   toc: no
  # html_document:
  #   toc: no
  #   df_print: paged
subtitle: Homework II
bibliography: references.bib
---


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# library(data.table)
library(tidyverse)
library(kableExtra)
library(fixest)
# library(grf)
# library(stargazer)
# library(texreg)
load("input/homework_II.RData")
```

# Part I

1. (10 points) Describe the conditions in which the differences-in-differences (DID) and the synthetic control (SC) estimators are valid. What are the main conditions in which these estimators are consistent/unbiased/asymptotically
unbiased?

DID:

Suppose we are in the simplest DID setup possible, units are separated in two states, $S_i\in\{0, 1\}$ and observed in two periods of time, $T_t\in \{0, 1\}$ and we also observe an outcome variable, $y_{it}$. We have the following potential outcomes model, with homogeneous treatment effect, $\beta$:

$$
\begin{cases}
y_{it}(0)=\alpha_0+S_i+T_t+\eta_{it}\\
y_{it}(1)=\beta+y_{it}(0)
\end{cases}
$$

This implies the following DID regression equation:


\begin{equation}
y_{it}=\alpha_0+S_i+T_t+\beta D_{it}+\eta_{it}
(\#eq:did)
\end{equation}

where $D_{it}\in\{0,1\}$ indicates the treatment status. If we let the treatment occur only at time $t=1$ for state $s=1$, then $D_{it}$ is the interaction of $S_i$ and $T_t$.

Equation regression above is a two-way fixed effect model and represents basic difference-in-difference model. The causal parameter of interest is $\beta$ and it corresponds to the double difference of $E[y_{i1}(1)-y_{i0}(1)|D=1]-E[y_{i1}(0)-y_{i0}(0)|D=0]$, thus, the estimator's name.

The causal parameter $\beta$ is identified under the assumption that **potential outcome trends would be the same** in both control and treatment groups, in the absence of treatment. It's the treatment that induces a deviation from this common (or parallel) trend.

This means that treatment assignment must be independent of potential outcomes, specifically, $E[D_{it}\eta_{it'}]=0$ for all $t$ and $t'$. **Treatment should not be chosen in response** to knowledge about how the outcome is likely to be and also that **outcome should not change in anticipation of treatment**.

SC:

The synthetic control method is mostly used when the units of observation are a small number of aggregate entities and there is one single treated unit, while a combination of untreated units provides a good approximation for the treat unit conterfactual. The formal setup of a SC method may be described as follows. 

We have at disposal observations for $J+1$ units and $j=0$ is our treated unit at times $t>t_0$ and $\mathcal{T}=\{1, \ldots, t_0, \ldots, T\}$. We call the "donor pool" the other untreated units, $j=1,\ldots, J$. We observe an outcome of interest for each unit at every period of time, $y_{jt}$. Suppose the outcome is explainable by a set of $k$ predictor variables, $X_{1j},\ldots, X_{kj}$. We can group the predictors variables for untreated units together and have a $k\times J$ matrix $\mathbf{X_0}$. Let the potential outcome for any unit $j$ without treatment at period $t$ be denoted by $y^N_{jt}$. The treated unit, after period $t_0$ will have suffered intervention, therefore its potential outcome under intervention is the observed outcome, $y^I_{1t}=Y_{1t}$ for $t>t_0$. We define the **effect of the intervention** on the outcome of interest as:

\begin{equation}
\tau_{1t}=Y^I_{1t}-Y^N_{1t}
(\#eq:sc-effect)
\end{equation}

for $t>t_0$. The challenge is to estimate the counterfactual $Y^N_{1t}, t>t_0$.

The SC approach can also be posed as a linear factor model of potential outcomes:

\begin{equation}
y_{jt}=\begin{cases}
    y_{jt}^N=c_j+\delta_t+\lambda_t\mu_j+\varepsilon_{jt}, \text{ if }d_{jt}=0\\
    y_{jt}^I=\alpha_{jt}+y_{jt}^N, \text{ if } d_{jt}=1
  \end{cases}
(\#eq:sc-factors)
\end{equation}
where $\lambda_t$ is the vector of common factors, $\mu_j$ the factor loadings, $\alpha_{jt}$ is the treatment effect and $\varepsilon_{jt}$ is the idiosyncratic shock. We must notice that common factors and respective loadings are **not observable** and are part of the error structure. The intervention is signaled by $d_{jt}=1$ and happens only when $j=0$ and $t>t_0$.  
 
The main assumptions are: i) the donor pool members are comparable to the treated unit and were not subject to treatment, and ii) predictors $X_{kj}$ are unaffected by the intervention and iii) the intervention itself is not related to idiosyncratic shocks, $\varepsilon_{jt}$, for every unity and all times.

If these assumptions are met, a synthetic control is defined as a weighted average of the units in the donor pool. Let $\mathbf{W}=(w_1,\ldots,w_{J})'$ be a $J\times 1$ vector of weights with all entries in $[0, 1]$ and sum to one. Given this set of weights we can estimate the counterfactual as:

\begin{align}
\hat{Y}^N_{1t}&=\sum_{j=2}^{J+1}w_jY_{jt}(\#eq:sc-ycounter)\\
\hat\tau_{1t}&=Y_{1t}-\hat{Y}^N_{1t}(\#eq:sc-tauhat)
\end{align}


From equation \@ref(eq:sc-ycounter) it is clear how the above assumptions play a role in giving an unbiased estimator for the effect of intervention. The expected value of $\hat{y}^N_{0t}$ must converge to the counterfactual $y^N_{0t}$, that is, the weighted average of synthetic control's outcome must be a good approximation of the treated outcome *had it not been treated*.


2. (10 points) Describe settings in which the conditions above would be invalid.

The treatment exogeneity may easily be violated in a DID setup, take for example @Chay2005 who consider a policy in Chile where poorly performing schools were given additional financial resources. The DID estimator compares average school outcomes between treated and control schools before and after the subsidy. It's easy to imagine that due to chance, an above threshold school had a poor performance exactly in the pre-treatment year, inducing this school to the treatment group. After receiving treatment, and suppose the true effect is negligible, the school has a normal performance above threshold as usual. In this case the DID estimator would overestimate the treatment effect in a phenomenon known as Ashenfelter's dip.

In the synthetic control framework one can think of a donor pool that is not close enough to the treated unit, that is, the treated unit is unique, and even when it had not suffered intervention it had unique characteristics that cannot be obtained by a weight average of all other units. @Abadie2020 presents some conditions on the context of the investigation under which synthetic controls are appropriate, and when it is not. The synthetic control method is not well suited when the outcome is highly volatile with respect to small shock, when the comparison group (i.e. donor pool) is not "free" from a similar shock we are interested and when there are large idiosyncratic shocks that affect their outcome.

Also, the synthetic control method, just like DID, does not hold if there is anticipation of treatment affecting the outcome prior to the intervention. Finally, the no interference, much like the SUTVA, must hold.

3. (10 points) Describe the different alternative inference methods that are available for these two estimators. Be specific about the conditions in which each inference method would be reliable.

Inference in DID estimation is typically carried out through asymptotic approximation. If available, the researcher can also make cluster robust and/or heteroskedasticity robust inference. Although, this approach is only available if the number of both treated and control units (or clusters) is large. When we have only a handful of treated units, @FermanPinto2019 show the CRVE estimator will underestimate the variance. Wild bootstrap in this framework would not be a good choice either, [@Canay2018].

When the number of treated units is small, but we do have a large number of controls in a DID framework, the method of @Conley2011 is the literature's preferred choice. But, it does come with strings attached. One of the method's assumptions is that errors are iid across unities and independent of treatment assignment, thus eliminating the possibility of heteroskedasticity across treatment and control groups. This assumption may be unrealistic in situation where the observations are aggregated from individual level and there is correlation among same group individuals. @FermanPinto2019 show this leads to a size problem in inference. They suggest an approach that estimates the conditional heteroskedastic variance and then rescales the residuals to apply the bootstrap procedure.

The synthetic control method is more problematic when it comes to inference. Results assessment is usually carried out by visual inspection and placebo tests. @Abadie2010 propose a mode of inference for the synthetic control framework that is based on permutation methods. Their method relies on a test statistic that measures the ratio of the post-intervention fit relative to the pre-intervention fit.

\begin{equation}
R_{j}\left(t_{1}, t_{2}\right)=\left(\frac{1}{t_{2}-t_{1}+1} \sum_{t=t_{1}}^{t_{2}}\left(Y_{j t}-\widehat{Y}_{j t}^{N}\right)^{2}\right)^{1 / 2}
(\#eq:rmspe)
\end{equation}

and the goodness-of-fit ratio is:

\begin{equation}
r_{j}=\frac{R_{j}\left(T_{0}+1, T\right)}{R_{j}\left(1, T_{0}\right)}
(\#eq:ratio)
\end{equation}
$r_j$ measures the quality of the fit of a synthetic control for unit j in the **post-treatment period**, relative to the quality of the fit in the pre-treatment period. @Abadie2010 use
the permutation distribution of $r_j$ for inference.

@chernozhukov2020exact develop permutation inference procedures that are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. The methods work in with many different approaches for predicting counterfactual mean outcomes in the absence of the intervention, like synthetic controls, difference-in-differences, etc. The authors draw their method from end-of-sample instability tests and devise a sampling-based inferential procedure for synthetic controls and related methods that employs permutations of regression residuals in the time dimension. The main assumption is errors stationarity and weak dependence on the counterfactual generating process. By imposing the null hypothesis of no intervention effect, a test based on permutations of the residuals, $\hat u_t = Y_{0t}-Y_{t}W^*$ can be carried on.

# Part II

Choose a published paper the uses a DID or SC estimator in which the dataset is available online. You can also consider a paper that uses shift-share designs, but in this case there is a risk that we will not cover that in class. Once you decide on your paper, sign up at this link. It must be a different paper for each student.

1. (10 points) Briefly summarize the research question in the paper, and why it is interesting. What is the main parameter of interest?

The chosen paper is @Kovak2013 which studies the impact of brazillian trade liberalization in the 1990s mainly under Collor government, to regional wages. The empirical strategy is designed as a shift-share experiment, since import tariffs are set at national level but its impact on local labor markets is dependent on the industries' share of labor demand.

How trade liberalization impacts economic growth has always been a relevant and debatable question, more recently the focus has passed to liberalization and inequality. Specially for developing countries, the literature finds that trade liberalization may result in increase of wage inequality, [@Goldberg2007].

@Kovak2013 follows a growing literature by examining the effects of trade liberalization on labor market outcomes at the subnational level. The author develops a theoretical specific-factors model of regional economies to back the literature's practice of measuring the local effect of
liberalization using a weighted average of changes in trade policy, with weights
based on the industrial distribution of labor in each region.

The main parameter of interest is the regional effect of trade liberalization on real wages. This is captured by the sensitivity of regional change of log-wages from 1990 to 1995 (the period considered so the liberalization run its course) to a measure of regional change in trade policy. The author calls this measure as the region-level tariff change -- RTC -- and it is described in the next question.

The empirical results confirm the model’s predictions. Local labor markets whose
workers were concentrated in industries facing the largest tariff cuts were generally affected more negatively.

2. (10 points) Briefly describe the empirical setting.

The shift-share "treatment" considered in the paper is the region-level tariff change (RTC) for each brazilian microregion computed as follows:

\begin{align}
RTC_r=&\sum_{i\neq N}\beta_{ri}d\ln(1+\tau_i)
(\#eq:rtc)\\
\text{where}\quad &\beta_{ri}=\frac{\lambda_{ri}1/\theta_i}{\sum_{i'\neq N}\lambda_{ri'}1/\theta_{i'}}
\end{align}

where $r$ denotes a brazilian microregion and $i$ an industry. Liberalization induced price changes from 1990 to 1995 is measured as $d\ln(1+\tau_i)$, where $\tau_i$ is the tariff rate. Parameter $\lambda_{ri}$ is the fraction of regional labor allocated to industry $i$ and $\theta_i$ is the cost share of industry-specific factor.

Therefore, the main specification for the empirical analysis of log-variation in wages to a liberalization shock is:

\begin{equation}
d\ln(w_r)=\zeta_0+\zeta_1RTC_r+\varepsilon_r
(\#eq:logwage)
\end{equation}

and the main parameter of interest is the regional effect of liberalization on real wages, $\zeta_1$.

Wage and employment data come from the brazilian demographic censuses (Censo Demográfico) for 1991 and 2000. Local labor markets were defined by microregions. Wages are calculated as earnings divided by hours for individuals aged 18-55 who are not enrolled in school. Industry classification comes from the Applied Economics Research Institute (IPEA), while @Kume2003 report nominal tariff change by industry during the liberalization.

The parameters for the fraction of regional labor allocated to an industry and industry-specific factor cost share are computed from 1991 census, taking into account individual level observations and adjusting wages by a Mincer-type estimation.

3. (10 points) Discuss whether the conditions for validity of the empirical method are reasonable.

The main identification assumption in regression \@ref(eq:logwage) is the strict exogeneity of the trade liberalization shock with respect to the industry performance. The analysis utilizes variation in tariff changes across industries to compose the regressor $RTC_r$. If the tariff reduction is correlated to industry performance, then one cannot argue the regressor in equation \@ref(eq:logwage) is independent from unobservable factors that also affect the variation in log-wages, hence, unconfoundedness does not hold and a causal interpretation of $\zeta_1$ is undue.

The author is aware of this assumption and dedicates a whole section to analyze why exogeneity is likely to hold. He qualitatively argues that the driving force of Brazil's liberalization came from government, not the private sector which had little to no influence on the process. A more striking support for exogeneity comes from the data on tariff cuts by industries. Industries most protect have experienced the largest cuts, showing that the primary goal of brazillian government was to reduce tariffs in general and making the cross-industry variation as least as possible considering external incentives.

Although the author makes a compelling case for exogeneity, it is not unrealistic to imagine some sort of industry lobbying during (the short, granted) reform discussion in Congress, for example. If that is the case, some industries may end up being relatively protected after the liberalization and this can be seen in Figure 1 from @Kovak2013 and shown bellow in figure \@ref(fig:fig1), where Agriculture; Footware, leather and Electric, electronic equipment; did not suffer a large reduction in tariff as expected by their preliberalization tariff level.

```{r fig1, fig.cap="Relationship between tariff changes and preliberalization tariff levels. Kovak (2013) Figure 1."}
knitr::include_graphics("kovak2013_fig_1.png")
```

Concern with the correct estimation of $\zeta_1$ is raised when the baseline data for employment and wages is contaminated with the "treatment". Ideally, wage and employment information to compose the baseline (the $RTC_r$ indeed) should come from just before liberalization began in March 1990. Although, the use 1991 census as the baseline period relies on the assumption that wages and employment shares adjusted slowly to the trade liberalization. The trade liberalization process itself is not a sharp, one-off event, and took place during almost half a decade. This opens the possibility that workers adapted during the period, for example getting qualification and changing employment to better paid jobs just before the 1991 census, thus changing the composition of labor share in a microregion **after** the shock has begun. This fact should create an attenuation bias.

Another source of attenuation bias is interregional mobility in response to liberalization, which will smooth out the regional wage variation that would have been observed on impact. Suppose the case of instant worker mobility, all liberalization-induced wage variation would be
immediately arbitraged away by worker migration. The author raises this potential problem, since the theoretical model predicts an increase in labor supply for regions facing increased wages, but do not demonstrate data that backs his assumption of negligible migration.

The baseline contamination and interregional labor migration may provoke **attenuation bias**, that is, $\zeta_1$ will tend towards zero. This kind of bias is not so restrictive in the present paper since if causal effect is statistically significant (and the author does get effect!) then the true effect would be even larger, hence, there would be no doubt over the findings (supposing the inference is correctly done, of course).

4. (10 points) Describe how the authors conducted inference. Discuss whether the conditions for validity of the inference method are reasonable.

Inference is carried out using cluster-robust variance estimation (CRVE) obtained from asymptotic theory. The unity of observation is microregion and those are clustered into states. The regression analysis counts with 493 observations clustered at 27 states. The number of clusters is borderline to what we expect to have reasonable asymptotic approximation, since CRVE valid when we have independence across clusters and the number of them grows in tandem with number of observations, thus $N_c\rightarrow\infty$.

In the next question I will perform the assessment proposed in @Ferman2019 to have an indication whether the approximation is reliable or not. After the assessment, other inference methods will be carried and compared to the paper's original one, like wild bootstrap, permutation test and standard error computation as in @Adao2019.

5. (30 points) Use the data from the paper to re-analyze the results of the paper. You can, for example, check whether the results are robust to alternative specifications or alternative approaches for inference, you can
provide some evidence that the assumptions the authors rely on are reasonable or unreasonable, you can check whether the inference method the authors are using is reliable, and so on. Be creative!

The author himself made several specifications to attest robustness and his theoretical findings. First let's replicate the author's main results table:

```{r etable-setup}
#' Setup fixest
setFixest_se(no_FE = "hetero")
setFixest_dof(dof(fixef.K = "full")) # get Stata's reg results

#' Dicionário para o nome das variáveis nas tabelas
fixest::setFixest_dict(c(rtc_main = "Regional tariff change", 
                         rtc_notheta = "Regional tariff change", 
                         rtc_nt = "Regional tariff change",
                         state = "State",
                         dlnwmmc_mincer = "log-variation in wage",
                         dlnwmmc_mincer_nt = "log-variation in wage"))
#' Ajusta o estilo das tabelas
# est_style = list(depvar = "title:Dep. Var.",
#                  model = "title:Modelo",
#                  var = "title:\\emph{Variáveis}",
#                  stats = "title:\\emph{Estatísticas de diagnóstico}",
#                  notes = "title:\\emph{\\medskip Notas:}")
```

```{r table1, results='asis'}
nt_checks <- c(rep(TRUE, 4), rep(FALSE, 2), rep(TRUE, 2))
labor_checks <- c(rep(TRUE, 2), rep(FALSE, 2), rep(TRUE, 4))
etable(tex = TRUE,
       reg_main, reg_main_fe, reg_nolab, reg_nolab_fe, reg_notrad, 
       reg_notrad_fe, reg_workers, reg_workers_fe,
       cluster = "state",
       digits = 3,
       title = "The effect of liberalization on local wages",
       label = "tab:table1",
       drop = "(Intercept)",
       yesNo = "$\\checkmark$",
       subtitles = c("Main", "Main", "No labor", "No labor",
                     "Nontraded", "Nontraded", "Workers", "Workers"),
       extraline = list("Nontraded sector Omited" = nt_checks,
                        "Zero price change" = !nt_checks,
                        "Labor share adjustment" = labor_checks))
```

Now that we have the same results as the original paper, we can make an assessment, as in @Ferman2019, to verify the conducted inference's reliability. Table \@ref(tab:ferman) provides the assessment for two different significance levels ($\alpha$ = 5% and 10%). 

```{r ferman, resutls='asis'}
kbl(assessment_tbl, digits = 3, booktabs = TRUE,
    caption = "Simple assessment on original inference method",
    col.names = c("Name", "$\\alpha = 0.05$", "$\\alpha = 0.10$"),
    row.names = TRUE,
    escape = FALSE) %>% 
  kable_classic(full_width = FALSE)
```

The assessment method is showing that specifications without a state fixed effect are less likely to suffer from overejection for both levels of significance. When state fixed effect is included, the overejection is exacerbated and there are serious concerns about CRVE reliability. As @Ferman2019 alerts, _"If this assessment uncovers a rejection rate significantly larger than the level of the test [...], then this would be a strong indication that the researcher should proceed with caution"_. Therefore, all fixed effect specifications still have rejection rates much higher than the test size and inference based on CRVE for models including fixed effects should be made with caution.

Having this concern in mind, we perform three alternative methods for inference for this particular study, the wild bootstrap, randomization inference and the methods proposed in @Adao2019.


```{r inferences, results='asis'}
kbl(inferences_df, digits = 4, booktabs = TRUE,
    caption = "Alternative methods for inference. p-values.",
    col.names = c("Name", "Unstud. p-value", 
                  "Stud. p-value", "p-value",
                  colnames(adao_df)[-1]),
    row.names = TRUE,
    escape = FALSE) %>% 
  kable_classic(full_width = FALSE) %>% 
  kable_styling(font_size = 9) %>% 
  add_header_above(c(" " = 2, "Wild Bootstrap" = 2, "RI" = 1, "Adao et. al." = 5))
```

Table \@ref(tab:inferences) provides **p-values** for all eight specifications under unstudentized and studentized wild bootstrap, randomization inference and, AKM and AKM0 methods. Homoskedastic, Eicker-Huber-White and CRVE (Region Cluster) are also provided for comparison purposes. Let's take as example the main specification with fixed effects, model number 2. In the original inference, CRVE, this parameter is deemed significant at 1% level, thus a p-value lower than 0.01, and this is indeed the case we found in Table \@ref(tab:inferences). The wild bootstrap method agrees with this value, while randomization inference (Fisher's exact p-value) shows a p-value of 0.021, and when AKM0 is used for inference this p-value jumps to 0.0657.

In general, for this particular paper, even though CRVE seems to be providing underestimated standard errors, especially when state fixed effects are included, the values are not very far from other inference methods and overall I would rather not say the inference is completely off-basis.

A robustness check is carried by introducing the average rate of unemployment in the baseline year of 1991 as a control variable. This data is not present in the original dataset, thus, I collected it from DATASUS and merged it to the data. Results are presented in Table \@ref(tab:robust).

```{r robust, resutls='asis'}
kbl(robust_df, digits = 4, booktabs = TRUE,
    caption = "Robustness check introducing average unemployment as control",
    col.names = c("Estimate", glue::glue("({n})", n = 1:8))) %>% 
  kable_classic(full_width = FALSE) %>% 
  add_header_above(c(" " = 1, "Main" = 2, "No labor" = 2, "Nontraded" = 2,
                     "Workers" = 2)) %>% 
  pack_rows("Standard Errors", 2, 6)
  
```

Table \@ref(tab:robust) shows the point estimate of a regional tariff change is reduced in magnitude but do not change sign. Most of the models do not show statistical significance for this parameter or it is at least reduced when compared to Table \@ref(tab:table1). Specifically for model (2), the paper's main model, CRVE standard error is 0.1779 on an estimate of 0.2461, while AKM standard error is 0.1025. 

\newpage
### Annex A - R Code for homework_II.R

```{r annex-a, code=readLines("homework_II.R"), eval = FALSE, echo=TRUE}

```

\newpage
### Annex B - R Code for homework_II_census.R

```{r annex-b, code=readLines("homework_II_census.R"), eval = FALSE, echo=TRUE}

```

\newpage

## References